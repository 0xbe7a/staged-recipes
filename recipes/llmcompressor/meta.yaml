{% set name = "llmcompressor" %}
{% set version = "0.5.2" %}

package:
  name: llmcompressor
  version: {{ version }}

source:
  url: https://pypi.org/packages/source/l/llmcompressor/llmcompressor-{{ version }}.tar.gz
  sha256: 97213268e16986c65817a58f79ec3fed3ea257f1cfaaed2560628c05f4d472b5

build:
  entry_points:
    - llmcompressor.trace=llmcompressor.transformers.tracing.debug:main
  noarch: python
  script:
    - export SETUPTOOLS_SCM_PRETEND_VERSION="$PKG_VERSION"   # [unix]
    - set "SETUPTOOLS_SCM_PRETEND_VERSION=%PKG_VERSION%"     # [win]
    - {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  host:
    - python {{ python_min }}.*
    - setuptools
    - wheel
    - setuptools-scm >8
    - pip
  run:
    - python >={{ python_min }}
    - loguru
    - pyyaml >=5.0.0
    - numpy >=1.17.0,<2.0
    - requests >=2.0.0
    - tqdm >=4.0.0
    - pytorch >=1.7.0
    - transformers >4.0,<5.0
    - datasets
    - accelerate >=0.20.3,!=1.1.0
    - pynvml
    - pillow
    - compressed-tensors >=0.10.2a2

test:
  imports:
    - llmcompressor
  commands:
    - pip check
    - llmcompressor.trace --help
  requires:
    - pip
    - python {{ python_min }}.*

about:
  home: https://github.com/vllm-project/llm-compressor
  summary: A library for compressing large language models utilizing the latest techniques and research in the field for both training aware and post training techniques. The library is designed to be flexible and easy to use on top of PyTorch and HuggingFace Transformers, allowing for quick experimentation.
  license: Apache-2.0
  license_file:
    - LICENSE
    - NOTICE

extra:
  recipe-maintainers:
    - timkpaine
