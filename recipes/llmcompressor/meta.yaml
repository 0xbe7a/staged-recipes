{% set name = "llmcompressor" %}
{% set version = "0.5.1" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.org/packages/source/{{ name[0] }}/{{ name }}/llmcompressor-{{ version }}.tar.gz
  sha256: 967d0a6eca093b7e4da0d76be826f74d0deb48f2c515761a16fb6d50d7791dd9

build:
  entry_points:
    - llmcompressor.trace=llmcompressor.transformers.tracing.debug:main
  noarch: python
  script:
    - export SETUPTOOLS_SCM_PRETEND_VERSION="$PKG_VERSION"   # [unix]
    - set "SETUPTOOLS_SCM_PRETEND_VERSION=%PKG_VERSION%"     # [win]
    - {{ PYTHON }} -m pip install . -vv --no-deps --no-build-isolation
  number: 0

requirements:
  host:
    - python >=3.9
    - setuptools
    - wheel
    - setuptools-scm >8
    - pip
  run:
    - python >=3.9
    - loguru
    - pyyaml >=5.0.0
    - numpy >=1.17.0,<2.0
    - requests >=2.0.0
    - tqdm >=4.0.0
    - pytorch >=1.7.0
    - transformers >4.0,<5.0
    - datasets
    - accelerate >=0.20.3,!=1.1.0
    - pynvml
    - pillow
    - compressed-tensors >=0.9.5a2

test:
  imports:
    - llmcompressor
  commands:
    - pip check
    - llmcompressor.trace --help
  requires:
    - pip

about:
  home: https://github.com/vllm-project/llm-compressor
  summary: A library for compressing large language models utilizing the latest techniques and research in the field for both training aware and post training techniques. The library is designed to be flexible and easy to use on top of PyTorch and HuggingFace Transformers, allowing for quick experimentation.
  license: Apache-2.0
  license_file:
    - LICENSE
    - NOTICE

extra:
  recipe-maintainers:
    - timkpaine
